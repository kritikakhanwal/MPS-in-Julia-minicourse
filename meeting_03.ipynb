{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMPS homework 1 template\n",
    "\n",
    "Let's use the following convention for numbering legs:\n",
    "```\n",
    " 1--A--3\n",
    "    |\n",
    "    2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using TensorOperations\n",
    "using LinearMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand_UMPS"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    rand_UMPS(d, D; keep_it_real=true)\n",
    "\n",
    "Return a random three-valent tensor A, that defines a uniform MPS (UMPS).\n",
    "The bond dimension of the physical leg should be d, and the bond dimension\n",
    "of the two \"virtual\" legs (the horizontal ones) should be D.\n",
    "keep_it_real is keyword argument, for whether the matrix should be real or\n",
    "complex.\n",
    "\n",
    "This means you can call\n",
    "`rand_UMPS(2, 9)`\n",
    "or\n",
    "`rand_UMPS(2, 9; keep_it_real=true)`\n",
    "and they both give a you real A, but you can also call\n",
    "`rand_UMPS(2, 9; keep_it_real=false)`\n",
    "to get a complex A.\n",
    "\"\"\"\n",
    "function rand_UMPS(d, D; keep_it_real=true)\n",
    "    shp = (D, d, D)\n",
    "    if keep_it_real\n",
    "        A = randn(shp)\n",
    "    else\n",
    "        A_real = randn(shp)\n",
    "        A_imag = randn(shp)\n",
    "        A = complex.(A_real, A_imag) / sqrt(2)\n",
    "    end\n",
    "    return A\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tm"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    tm(A)\n",
    "\n",
    "Return the transfer matrix of A:\n",
    " i1--A---j1\n",
    "     |  \n",
    " i2--A*--j2\n",
    "\"\"\"\n",
    "function tm(A)\n",
    "    @tensor T[i1,i2,j1,j2] := A[i1,p,j1]*conj(A)[i2,p,j2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tm_eigs_dense"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function eig_and_trunc(T, nev)\n",
    "    S, U = eig(T)\n",
    "    perm = sortperm(S; by=abs, rev=true)\n",
    "    S = S[perm]\n",
    "    U = U[:, perm]\n",
    "    S = S[1:nev]\n",
    "    U = U[:, 1:nev]\n",
    "    return S, U\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    tm_eigs(A, dirn, nev)\n",
    "\n",
    "Return some of the eigenvalues and vectors of the transfer matrix of A.\n",
    "dirn should be \"L\", \"R\" or \"BOTH\", and determines which eigenvectors to return.\n",
    "nev is the number of eigenpairs to return (starting with the eigenvalues with\n",
    "largest magnitude).\n",
    "\"\"\"\n",
    "function tm_eigs_dense(A, dirn, nev)\n",
    "    T = tm(A)\n",
    "    D = size(T, 1)\n",
    "    T = reshape(T, (D^2, D^2))\n",
    "    nev = min(nev, D^2)\n",
    "    \n",
    "    result = ()\n",
    "    if dirn == \"R\" || dirn == \"BOTH\"\n",
    "        SR, UR = eig_and_trunc(T, nev)\n",
    "        UR = [reshape(UR[:,i], (D, D)) for i in 1:nev]\n",
    "        result = tuple(result..., SR, UR)\n",
    "    end\n",
    "    if dirn == \"L\" || dirn == \"BOTH\"\n",
    "        SL, UL = eig_and_trunc(T', nev)\n",
    "        UL = [reshape(UL[:,i], (D, D)) for i in 1:nev]\n",
    "        result = tuple(result..., SL, UL)\n",
    "    end\n",
    "    return result\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    normalize!(A)\n",
    "\n",
    "Normalize the UMPS defined by A. This is done by dividing A by the square root of\n",
    "the dominant (largest magnitude) eigenvalue of the MPS transfer matrix.\n",
    "\"\"\"\n",
    "function normalize!(A)\n",
    "    S, U = tm_eigs_dense(A, \"R\", 1)\n",
    "    S1 = S[1]\n",
    "    A ./= sqrt(S1)\n",
    "    return A\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR[1] = 20.525050253965077 - 1.2434497875801753e-14im\n",
      "SL[1] = 20.52505025396496 - 8.386786882106903e-15im\n",
      "SR[1] = 1.0000000000000029 - 2.220446049250313e-16im\n",
      "SL[1] = 0.9999999999999956 - 8.875991479296312e-16im\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999956 - 8.875991479296312e-16im"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    d = 2\n",
    "    D = 10\n",
    "    A = rand_UMPS(d, D; keep_it_real=false)\n",
    "\n",
    "    SR, UR, SL, UL = tm_eigs_dense(A, \"BOTH\", 1)\n",
    "    @show SR[1]\n",
    "    @show SL[1]\n",
    "    normalize!(A)\n",
    "    SR, UR, SL, UL = tm_eigs_dense(A, \"BOTH\", 1)\n",
    "    @show SR[1]\n",
    "    @show SL[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurray!\n",
    "Now let's get smart about diagonalizing that MPS transfer matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tm_r"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    tm_l(A, x)\n",
    "\n",
    "Return y, where\n",
    "/------   /------A--\n",
    "|       = |      |  \n",
    "\\- y* -   \\- x* -A*-\n",
    "\"\"\"\n",
    "function tm_l(A, x)\n",
    "    @tensor y[i, j] := (x[a, b] * A[b, p, j]) * conj(A[a, p, i])\n",
    "    return y\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    tm_r(A, x)\n",
    "\n",
    "Return y, where\n",
    "-- y -\\   --A-- x -\\\n",
    "      | =   |      |\n",
    "------/   --A*-----/\n",
    "\"\"\"\n",
    "function tm_r(A, x)\n",
    "    @tensor y[i, j] := A[i, p, a] * (conj(A[j, p, b]) * x[a, b])\n",
    "    return y\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tm_eigs_sparse (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tm_eigs_sparse(A, dirn, nev)\n",
    "    if dirn == \"BOTH\"\n",
    "        SR, UR = tm_eigs_sparse(A, \"R\", nev)\n",
    "        SL, UL = tm_eigs_sparse(A, \"L\", nev)\n",
    "        return SR, UR, SL, UL\n",
    "    else\n",
    "        D = size(A, 1)\n",
    "        x = zeros(eltype(A), (D, D))\n",
    "        if dirn == \"L\"\n",
    "            f = v -> vec(tm_l(A, copy!(x, v)))\n",
    "        else\n",
    "            f = v -> vec(tm_r(A, copy!(x, v)))\n",
    "        end\n",
    "\n",
    "        fmap = LinearMap{eltype(A)}(f, D^2)\n",
    "        S, U, nconv, niter, nmult, resid = eigs(fmap, nev=nev, which=:LM, ritzvec=true)\n",
    "        U = [reshape(U[:,i], (D, D)) for i in 1:size(U, 2)]\n",
    "\n",
    "        return S, U\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.475528 seconds (2.32 M allocations: 292.103 MiB, 1.69% gc time)\n",
      " 21.932633 seconds (194 allocations: 196.494 MiB, 0.53% gc time)\n",
      "\n",
      "Comparison of eigenvalues:\n",
      "Complex{Float64}[80.8867+1.24345e-14im, -6.41722-58.6043im]\n",
      "Complex{Float64}[80.8867+1.96076e-14im, -6.41722-58.6043im]\n",
      "\n",
      "Comparison of eigenvectors:\n",
      "Complex{Float64}[0.13274+3.1225e-17im, 0.013985+0.00483383im, -0.00543919+0.0201979im, 0.0190075+0.0029064im, -0.000825505+0.00591032im, 0.0106425+0.00524406im]\n",
      "Complex{Float64}[-0.112287+0.0707916im, -0.0144081+0.00336933im, -0.00617067-0.0199866im, -0.0176289+0.00767835im, -0.00245373-0.00543991im, -0.0117994+0.0012397im]\n",
      "\n",
      "Comparison of abs.(eigenvectors):\n",
      "[0.13274, 0.0147968, 0.0209175, 0.0192285, 0.0059677, 0.0118643]\n",
      "[0.13274, 0.0147968, 0.0209175, 0.0192285, 0.0059677, 0.0118643]\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    d = 2\n",
    "    D = 40\n",
    "    A = rand_UMPS(d, D; keep_it_real=false)\n",
    "\n",
    "    nev = 2\n",
    "    @time SR_sparse, UR_sparse = tm_eigs_sparse(A, \"R\", nev)\n",
    "    @time SR_dense, UR_dense = tm_eigs_dense(A, \"R\", nev)\n",
    "    println(\"\\nComparison of eigenvalues:\")\n",
    "    println(SR_dense)\n",
    "    println(SR_sparse)\n",
    "    println(\"\\nComparison of eigenvectors:\")\n",
    "    println(UR_dense[1][1:6])\n",
    "    println(UR_sparse[1][1:6])\n",
    "    println(\"\\nComparison of abs.(eigenvectors):\")\n",
    "    println(abs.(UR_dense[1])[1:6])\n",
    "    println(abs.(UR_sparse[1])[1:6])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tm_eigs (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tm_eigs(A, dirn, nev; max_dense_D=10)\n",
    "    D = size(A, 1)\n",
    "    if D <= max_dense_D || nev >= D^2\n",
    "        return tm_eigs_dense(A, dirn, nev)\n",
    "    else\n",
    "        return tm_eigs_sparse(A, dirn, nev)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onwards to new things: Computing expectation values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation value of an operator $O$, for a generic quantum state $|\\psi\\rangle$, is $\\frac{\\langle \\psi | O | \\psi \\rangle}{\\langle \\psi | \\psi \\rangle}$. Since we have now normalized our MPS state, we can ignore the denominator, and concentrate on $\\langle \\psi | O | \\psi \\rangle$.\n",
    "\n",
    "For a UMPS state $| \\text{UMPS}(A) \\rangle$ and a single site operator $O$ this expectation value can be computed as follows:\n",
    "\n",
    "<img src=\"fig/expect_local.svg\">\n",
    "\n",
    "We have made use here of the fact that we're always living at the limit $N \\to \\infty$, and thus $T^N = |r_1\\rangle \\lambda_1^N \\langle l_1|$, where $\\lambda_1$ is the dominant eigenvalue, which we further know is $1$.\n",
    "\n",
    "Similarly, the expectation value of the tensor product of two one-site operators, i.e. a two-point correlator, can be computed as follows:\n",
    "\n",
    "<img src=\"fig/correlator_twopoint.svg\">\n",
    "\n",
    "Here $O_1^{(0)}$ is located on site $0$ and $O_1^{(m)}$ is located on site $m$ (note that everything is translation invariant, so this is fully general).\n",
    "\n",
    "This seems simple enough! First, we need to get the dominant left and right eigenvectors of $T$, $|r_1\\rangle$ and $\\langle l_1|$. That we are already doing when implementing the normalization. Second, we need a function that applies $T_O$ to a vector in the $D^2$ dimensional virtual vector space, i.e., a function like `tm_l` and `tm_r`, but now with an operator $O$ included in the middle. (Throughout all this, I recommend thinking of the vectors in the $D^2$ dimensional space as $D \\times D$ matrices, as for instance is done in the functions `tm_r` and `tm_l`.) Using such a function, we can first evaluate for instance $T_{O_2} | r_1 \\rangle = |v \\rangle$, then multiply $|v \\rangle$ by $T$ $m-1$ times, and finally once by $T_{O_1}$. At the end we are left with something like $\\langle l_1| v \\rangle$, which is just an inner product in the $D^2$ dimensional vector space:\n",
    "\n",
    "<img src=\"fig/D2_inner_product.svg\">\n",
    "\n",
    "Of course, we could also start the contraction from the left instead.\n",
    "\n",
    "In light of this, your homework would naturally be to implement functions with the following signatures:\n",
    "\n",
    "`tm_r_op(A, O, x)`: Transfer matrix with an operator, multiplied from the right.\n",
    "\n",
    "`tm_l_op(A, O, x)`: Transfer matrix with an operator, multiplied from the left.\n",
    "\n",
    "`expect_local(A, O)`: Expectation value of $O$.\n",
    "\n",
    "`correlator_twopoint(A, O1, O2, m)`: Two-point correlators between two operators, distance $m$ apart. You should compute the connected two-point correlators, i.e., $\\langle O_1^{(0)} O_2^{(m)} \\rangle - \\langle O_1 \\rangle \\langle O_2 \\rangle$. In addition, since on the way to computing the two-point correlator for distance $m$, you end computing all the two-point correlators for distance $i \\leq m$, you should make the function return all of them, because why not.\n",
    "\n",
    "However, before you jump into writing code, there's a slight additional complication. Since we often want to decompose $T$ as $T = \\sum_{i=1}^{D^2} |r_i \\rangle \\lambda_i \\langle l_i|$, we need to make sure that not only are the $|r_i \\rangle$ and $\\langle l_i|$ we have eigenvectors of $T$, but also that they are normalized so that $\\langle l_i| r_j \\rangle = \\delta_{ij}$. If we do not impose this, there's no guarantee that the eigenvectors returned by `eigs` or `eig` have this property: $\\langle l_i| r_j \\rangle$ will be diagonal, but the values on the diagonal are not necessarily $1$, unless we scale the eigenvectors ourselves to force this. We'll only really ever need $|r_1 \\rangle$ and $\\langle l_1|$, i.e., the dominant ones, and we are already computing them as part of the normalization process. Thus it makes sense to modify the `normalize!` function so that, in addition to normalizing the MPS, it also normalizes the dominant eigenvectors, and returns them. I've written such a modified `normalize!` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'normalize! :: Tuple{Any}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "normalize!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    normalize!(A)\n",
    "\n",
    "Normalize the UMPS defined by A, and return the dominant left and right\n",
    "eigenvectors l and r of its transfer matrix, normalized so that l'*r = 1.\n",
    "\"\"\"\n",
    "function normalize!(A)\n",
    "    SR, UR, SL, UL = tm_eigs(A, \"BOTH\", 1)\n",
    "    S1 = SR[1]\n",
    "    A ./= sqrt(S1)  # Normalizing the state.\n",
    "    \n",
    "    # Normalizing the eigenvectors\n",
    "    l = UL[1]\n",
    "    r = UR[1]  \n",
    "    #We need this to be 1\n",
    "    n = vec(l)'*vec(r)\n",
    "    abs_n = abs(n)\n",
    "    phase_n = abs_n/n\n",
    "    sfac = 1.0/sqrt(abs_n)\n",
    "    l .*= sfac/phase_n\n",
    "    r .*= sfac\n",
    "    return l, r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell should have overridden the previous definition of `normalize!` that we had.\n",
    "\n",
    "Let's just quickly confirm that this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    d = 2\n",
    "    D = 20\n",
    "    A = rand_UMPS(d, D; keep_it_real=false)\n",
    "    \n",
    "    l, r = normalize!(A)\n",
    "    SR, UR, SL, UL = tm_eigs(A, \"BOTH\", 1)\n",
    "    println(SR[1] ≈ 1.0)\n",
    "    println(SL[1] ≈ 1.0)\n",
    "    \n",
    "    println(l ≈ tm_l(A, l))\n",
    "    println(r ≈ tm_r(A, r))\n",
    "    println(vec(l)'*vec(r) ≈ 1.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(By the way, I'm putting some of the code in these `let`-`end` blocks. That's to prevent names that I define in these blocks from polluting the global namespace: If I define `d = 2` within the `let`-`end` block, `d` will still be an unassigned name outside the block. Just a bit of basic code hygiene, when working with notebooks.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this new `normalize!` function, we are all set to implement computing expectation values. The natural way to proceed is to call `normalize!` once on an MPS after you create or modify it, and store the dominant eigenvectors that `normalize!` returns (I call them `l` and `r` in the code). These eigenvectors should then be passed to the functions that need them, so that they don't need to recompute them.\n",
    "\n",
    "Below I've made empty template functions to compute the expectation values we were talking about above. Your job is to fill in the blanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expect_local"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    expect_local(A, O, l, r)\n",
    "\n",
    "Return the expectation value of the one-site operator O for the (normalized)\n",
    "UMPS state defined by the tensor A. l and r should be the dominant\n",
    "eigenvectors of the MPS transfer matrix of A.\n",
    "\"\"\"\n",
    "function expect_local(A, O, l, r)\n",
    "    # ???\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mreplacing docs for 'correlator_twopoint :: NTuple{6,Any}' in module 'Main'.\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "correlator_twopoint"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    correlator_twopoint(A, O1, O2, m, l, r)\n",
    "\n",
    "Return the (connected) two-point correlator of operators O1 and O2 for the\n",
    "state UMPS(A), when O1 and O2 are i sites apart, where i ranges from 1 to m. In\n",
    "other words, return <O1^(0) O2^(i)> - <O1> <O2>, for all i = 1,...,m, where the\n",
    "expectation values are with respect to the state |UMPS(A)>. l and r should be\n",
    "the dominant eigenvectors of the MPS transfer matrix of A.\n",
    "\"\"\"\n",
    "function correlator_twopoint(A, O1, O2, m, l, r)\n",
    "    # ???\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tm_r_op"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    tm_l_op(A, O, x)\n",
    "\n",
    "Return y, where\n",
    "/------   /------A--\n",
    "|         |      |  \n",
    "|       = |      O  \n",
    "|         |      |  \n",
    "\\- y* -   \\- x* -A*-\n",
    "\"\"\"\n",
    "function tm_l_op(A, O, x)\n",
    "    # ???\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    tm_r_op(A, O, x)\n",
    "\n",
    "Return y, where\n",
    "-- y -\\   --A-- x -\\\n",
    "      |     |      |\n",
    "      | =   O      |\n",
    "      |     |      |\n",
    "------/   --A*-----/t\n",
    "\"\"\"\n",
    "function tm_r_op(A, O, x)\n",
    "    # ???\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ha, you thought you were done!\n",
    "\n",
    "Let's take a look at the expression for the connected two-point correlator:\n",
    "$\\langle O_1^{(0)} O_2^{(m)} \\rangle - \\langle O_1 \\rangle \\langle O_2 \\rangle\n",
    "= \\langle l_1 | T_{O_1}  T^{m-1} T_{O_2} | r_1 \\rangle$<br>\n",
    "Consider what happens when $m$ is large. Then we can also decompose<br>\n",
    "$T^{m-1} = \\sum_{i=1}^{D^2} |r_i \\rangle \\lambda_i^{m-1} \\langle l_i|\n",
    "\\approx |r_1 \\rangle \\langle l_1|$.<br>\n",
    "Plugging this into the above equation,\n",
    "$\\langle O_1^{(0)} O_2^{(m)} \\rangle - \\langle O_1 \\rangle \\langle O_2 \\rangle\n",
    "\\approx \\langle l_1 | T_{O_1} |r_1 \\rangle \\langle l_1| T_{O_2} |r_1 \\rangle - \\langle O_1 \\rangle \\langle O_2 \\rangle = \\langle O_1 \\rangle \\langle O_2 \\rangle - \\langle O_1 \\rangle \\langle O_2 \\rangle = 0.$<br>\n",
    "Well duh, we've discovered that at large distances the connected correlation function goes to zero. We can get something more interesting if we also take into account the second largest eigenvalue of $T$:\n",
    "$T^{m-1} \\approx |r_1 \\rangle \\langle l_1| + |r_2 \\rangle \\lambda_2^{m-1} \\langle l_2|$<br>\n",
    "$\\Rightarrow\n",
    "\\langle O_1^{(0)} O_2^{(m)} \\rangle - \\langle O_1 \\rangle \\langle O_2 \\rangle\n",
    "\\approx \\langle l_1 | T_{O_1} |r_2 \\rangle \\lambda_2^{m-1} \\langle l_2| T_{O_2} |r_1 \\rangle\n",
    "= C(A, O_1, O_2) ~ \\phi(A, m) ~ |\\lambda_2|^m.$<br>\n",
    "Here $C$ is the part with the inner products that doesn't depend on $m$ at all, and $\\phi$ is some phase that may depend on $m$ (if $O_1$ and $O_2$ are Hermitian operators $\\phi$ is just a sign). The magnitude of the correlation function depends on the distance as\n",
    "$|\\lambda_2|^m$, which we rewrite as $e^{m \\ln |\\lambda_2|} = e^{- \\frac{m}{\\xi}}$, with $\\xi = -\\frac{1}{\\ln |\\lambda_2|}$.\n",
    "\n",
    "In other words, for an MPS (for *any* MPS!), at large distances, two-point correlators always decay exponentially, with a *correlation length* $\\xi = -\\frac{1}{\\ln |\\lambda_2|}$, where $\\lambda_2$ is the second largest eigenvalue of the MPS transfer matrix (by absolute value).\n",
    "\n",
    "Now that's a good thing to know! For one, the correlation length of a state is sometimes of physical interest in itself. In addition, this gives a very useful characterization of the kinds of states MPS can represent: If you want to represent a state as an MPS, the state better have exponential decay of correlators, or you're in trouble. As you can probably guess, low-energy states of local Hamiltonians typically have this behavior (except for gapless Hamiltonians).\n",
    "\n",
    "Given this, as a final piece, you should try to implement the function below that computes the correlation length. You could then even compare the two-point correlators you are computing numerically, to check that they really do decay like $e^{- \\frac{m}{\\xi}}$. A good idea would be for instance to plot the numerical data against $e^{- \\frac{m}{\\xi}}$. We can look into some basic plotting next time, but if you want to go ahead, you can try starting here: https://julialang.org/downloads/plotting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correlation_length"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    correlation_length(A)\n",
    "\n",
    "Return the correlation length ξ of the (normalized) UMPS defined by A.\n",
    "\"\"\"\n",
    "function correlation_length(A)\n",
    "    # ???\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
